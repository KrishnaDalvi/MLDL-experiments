# ==============================
# EXPERIMENT
# Decision Tree & Random Forest
# Breast Cancer Classification
# ==============================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    roc_curve,
    auc
)

# --------------------------
# Load Dataset
# --------------------------
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

print("Dataset Shape:", X.shape)

# --------------------------
# Train-Test Split
# --------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# --------------------------
# Feature Scaling
# --------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ==========================
# Decision Tree
# ==========================
dt = DecisionTreeClassifier(max_depth=5, random_state=42)
dt.fit(X_train_scaled, y_train)

dt_preds = dt.predict(X_test_scaled)
dt_probs = dt.predict_proba(X_test_scaled)[:, 1]

print("\n--- Decision Tree ---")
print("Accuracy:", accuracy_score(y_test, dt_preds))
print(classification_report(y_test, dt_preds))

# ==========================
# Random Forest
# ==========================
rf = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42)
rf.fit(X_train_scaled, y_train)

rf_preds = rf.predict(X_test_scaled)
rf_probs = rf.predict_proba(X_test_scaled)[:, 1]

print("\n--- Random Forest ---")
print("Accuracy:", accuracy_score(y_test, rf_preds))
print(classification_report(y_test, rf_preds))

# ==========================
# Confusion Matrices
# ==========================
cm_dt = confusion_matrix(y_test, dt_preds)
cm_rf = confusion_matrix(y_test, rf_preds)

plt.figure(figsize=(10,4))

plt.subplot(1,2,1)
plt.imshow(cm_dt)
plt.title("Decision Tree Confusion Matrix")
plt.colorbar()
for i in range(cm_dt.shape[0]):
    for j in range(cm_dt.shape[1]):
        plt.text(j, i, cm_dt[i, j], ha="center", va="center")

plt.subplot(1,2,2)
plt.imshow(cm_rf)
plt.title("Random Forest Confusion Matrix")
plt.colorbar()
for i in range(cm_rf.shape[0]):
    for j in range(cm_rf.shape[1]):
        plt.text(j, i, cm_rf[i, j], ha="center", va="center")

plt.tight_layout()
plt.show()

# ==========================
# ROC Curve
# ==========================
dt_fpr, dt_tpr, _ = roc_curve(y_test, dt_probs)
rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)

plt.figure()
plt.plot(dt_fpr, dt_tpr, label=f"DT AUC = {auc(dt_fpr, dt_tpr):.2f}")
plt.plot(rf_fpr, rf_tpr, label=f"RF AUC = {auc(rf_fpr, rf_tpr):.2f}")
plt.plot([0,1],[0,1],'--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison")
plt.legend()
plt.show()

# ==========================
# Feature Importance (RF)
# ==========================
importances = rf.feature_importances_
indices = np.argsort(importances)[-10:]

plt.figure()
plt.barh(X.columns[indices], importances[indices])
plt.title("Top 10 Feature Importance (Random Forest)")
plt.show()

# ==========================
# Decision Tree Visualization
# ==========================
plt.figure(figsize=(18,8))
plot_tree(dt,
          feature_names=X.columns,
          class_names=["Malignant", "Benign"],
          filled=True)
plt.show()
